% This file was created with Citavi 6.4.0.35

@proceedings{.0507July2006,
 year = {05-07 July 2006},
 title = {Tenth International Conference on Information Visualisation (IV'06)},
 publisher = {IEEE},
 isbn = {0-7695-2602-0}
}


@proceedings{.1924Oct.1997,
 year = {19-24 Oct. 1997},
 title = {Proceedings. Visualization '97 (Cat. No. 97CB36155)},
 publisher = {IEEE},
 isbn = {0-8186-8262-0}
}


@proceedings{.2000,
 year = {2000},
 title = {In Proceedings of the IEEE Information Visualization Symposium, Late Breaking Hot Topics}
}


@book{.2005,
 year = {2005},
 title = {Visualization Handbook},
 publisher = {Elsevier},
 isbn = {9780123875822}
}


@book{.2015,
 year = {2015},
 title = {International Encyclopedia of the Social {\&} Behavioral Sciences},
 publisher = {Elsevier},
 isbn = {9780080970875}
}


@inproceedings{ANKERST.2000,
 author = {Ankerst, Mihael and Ester, Martin and Kriegel, Hans-Peter},
 title = {Towards an effective cooperation of the user and the computer for classification},
 pages = {179--188},
 publisher = {{ACM Press}},
 isbn = {1581132336},
 editor = {Ramakrishnan, Raghu and Stolfo, Sal and Bayardo, Roberto and Parsa, Ismail},
 booktitle = {Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining  - KDD '00},
 year = {2000},
 address = {New York, New York, USA},
 doi = {10.1145/347090.347124},
 file = {http://portal.acm.org/citation.cfm?doid=347090}
}


@article{Asimov.1985,
 author = {Asimov, Daniel},
 year = {1985},
 title = {The Grand Tour: A Tool for Viewing Multidimensional Data},
 pages = {128--143},
 volume = {6},
 number = {1},
 issn = {0196-5204},
 journal = {SIAM Journal on Scientific and Statistical Computing},
 doi = {10.1137/0906011}
}


@proceedings{Barbara.2003,
 year = {2003},
 title = {Proceedings of the 2003 SIAM International Conference on Data Mining},
 address = {Philadelphia, PA},
 publisher = {{Society for Industrial and Applied Mathematics}},
 isbn = {978-0-89871-545-3},
 editor = {Barbara, Daniel and Kamath, Chandrika},
 doi = {10.1137/1.9781611972733}
}


@article{Belkin.2003,
 author = {Belkin, Mikhail and Niyogi, Partha},
 year = {2003},
 title = {Laplacian Eigenmaps for Dimensionality Reduction and Data Representation},
 pages = {1373--1396},
 volume = {15},
 number = {6},
 issn = {0899-7667},
 journal = {Neural Computation},
 doi = {10.1162/089976603321780317}
}


@misc{Bie.,
 abstract = {Methods for Projection Pursuit aim to facilitate the visual exploration of high-dimensional data by identifying interesting low-dimensional projections. A major challenge is the design of a suitable quality metric of projections, commonly referred to as the projection index, to be maximized by the Projection Pursuit algorithm. In this paper, we introduce a new information-theoretic strategy for tackling this problem, based on quantifying the amount of information the projection conveys to a user given their prior beliefs about the data. The resulting projection index is a subjective quantity, explicitly dependent on the intended user. As a useful illustration, we developed this idea for two particular kinds of prior beliefs. The first kind leads to PCA (Principal Component Analysis), shining new light on when PCA is (not) appropriate. The second kind leads to a novel projection index, the maximization of which can be regarded as a robust variant of PCA. We show how this projection index, though non-convex, can be effectively maximized using a modified power method as well as using a semidefinite programming relaxation. The usefulness of this new projection index is demonstrated in comparative empirical experiments against PCA and a popular Projection Pursuit method.},
 author = {de Bie, Tijl and Lijffijt, Jefrey and Santos-Rodriguez, Raul and Kang, Bo},
 title = {Informative Data Projections: A Framework and Two Examples},
 publisher = {arXiv},
 doi = {10.48550/arXiv.1511.08762}
}


@incollection{Blum.2008,
 author = {Blum, Christian and Roli, Andrea},
 title = {Hybrid Metaheuristics: An Introduction},
 pages = {1--30},
 volume = {114},
 publisher = {{Springer Berlin Heidelberg}},
 isbn = {978-3-540-78294-0},
 series = {Studies in Computational Intelligence},
 editor = {Kacprzyk, Janusz and Blum, Christian and Aguilera, Maria Jos{\'e} Blesa and Roli, Andrea and Sampels, Michael},
 booktitle = {Hybrid Metaheuristics},
 year = {2008},
 address = {Berlin, Heidelberg},
 doi = {10.1007/978-3-540-78295-7{\textunderscore }1}
}


@article{Chu.2006,
 abstract = {This paper proposes a novel real-time electromyogram (EMG) pattern recognition for the control of a multifunction myoelectric hand from four channel EMG signals. To extract a feature vector from the EMG signal, we use a wavelet packet transform that is a generalized version of wavelet transform. For dimensionality reduction and nonlinear mapping of the features, we also propose a linear-nonlinear feature projection composed of principal components analysis (PCA) and a self-organizing feature map (SOFM). The dimensionality reduction by PCA simplifies the structure of the classifier and reduces processing time for the pattern recognition. The nonlinear mapping by SOFM transforms the PCA-reduced features into a new feature space with high class separability. Finally, a multilayer perceptron (MLP) is used as the classifier. Using an analysis of class separability by feature projections, we show that the recognition accuracy depends more on the class separability of the projected features than on the MLP's class separation ability. Consequently, the proposed linear-nonlinear projection method improves class separability and recognition accuracy. We implement a real-time control system for a multifunction virtual hand. Our experimental results show that all processes, including virtual hand control, are completed within 125 ms, and the proposed method is applicable to real-time myoelectric hand control without an operational time delay.},
 author = {Chu, Jun-Uk and Moon, Inhyuk and Mun, Mu-Seong},
 year = {2006},
 title = {A real-time EMG pattern recognition system based on linear-nonlinear feature projection for a multifunction myoelectric hand},
 pages = {2232--2239},
 volume = {53},
 number = {11},
 journal = {IEEE transactions on bio-medical engineering},
 doi = {10.1109/TBME.2006.883695},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/17073328}
}


@article{Cook.1995,
 author = {Cook, Dianne and Buja, Andreas and Cabrera, Javier and Hurley, Catherine},
 year = {1995},
 title = {Grand Tour and Projection Pursuit},
 pages = {155},
 volume = {4},
 number = {3},
 issn = {10618600},
 journal = {Journal of Computational and Graphical Statistics},
 doi = {10.2307/1390844}
}


@article{Daniels.2012,
 author = {Daniels, Karen and Grinstein, Georges and Russell, Adam and Glidden, Mason},
 year = {2012},
 title = {Properties of normalized radial visualizations},
 pages = {273--300},
 volume = {11},
 number = {4},
 issn = {1473-8716},
 journal = {Information visualization},
 doi = {10.1177/1473871612439357}
}


@article{Demartines.1997,
 abstract = {We present a new strategy called {\textquotedbl}curvilinear component analysis{\textquotedbl} (CCA) for dimensionality reduction and representation of multidimensional data sets. The principle of CCA is a self-organized neural network performing two tasks: vector quantization (VQ) of the submanifold in the data set (input space); and nonlinear projection (P) of these quantizing vectors toward an output space, providing a revealing unfolding of the submanifold. After learning, the network has the ability to continuously map any new point from one space into another: forward mapping of new points in the input space, or backward mapping of an arbitrary position in the output space.},
 author = {Demartines, P. and Herault, J.},
 year = {1997},
 title = {Curvilinear component analysis: a self-organizing neural network for nonlinear mapping of data sets},
 pages = {148--154},
 volume = {8},
 number = {1},
 issn = {1045-9227},
 journal = {IEEE transactions on neural networks},
 doi = {10.1109/72.554199},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/18255618}
}


@misc{Dua.2017,
 author = {Dua, Dheeru and Graff, Casey},
 year = {2017},
 title = {UCI Machine Learning Repository},
 url = {http://archive.ics.uci.edu/ml},
 institution = {{University of California, Irvine, School of Information and Computer Sciences{\textquotedbl}}}
}


@misc{Frimodig.2022,
 author = {Frimodig, B.},
 editor = {{Simply Psychology}},
 year = {2022},
 title = {What are Heuristics},
 url = {https://www.simplypsychology.org/what-is-a-heuristic.html},
 urldate = {06.07.2022}
}


@incollection{Gallardo.2005,
 author = {Gallardo, Jos{\'e} E. and Cotta, Carlos and Fern{\'a}ndez, Antonio J.},
 title = {Solving the Multidimensional Knapsack Problem Using an Evolutionary Algorithm Hybridized with Branch and Bound},
 pages = {21--30},
 volume = {3562},
 publisher = {{Springer Berlin Heidelberg}},
 isbn = {978-3-540-26319-7},
 series = {Lecture Notes in Computer Science},
 editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and {Pandu Rangan}, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Mira, Jos{\'e} and {\'A}lvarez, Jos{\'e} R.},
 booktitle = {Artificial Intelligence and Knowledge Engineering Applications: A Bioinspired Approach},
 year = {2005},
 address = {Berlin, Heidelberg},
 doi = {10.1007/11499305{\textunderscore }3}
}


@misc{Gu.2012,
 abstract = {Fisher score is one of the most widely used supervised feature selection methods. However, it selects each feature independently according to their scores under the Fisher criterion, which leads to a suboptimal subset of features. In this paper, we present a generalized Fisher score to jointly select features. It aims at finding an subset of features, which maximize the lower bound of traditional Fisher score. The resulting feature selection problem is a mixed integer programming, which can be reformulated as a quadratically constrained linear programming (QCLP). It is solved by cutting plane algorithm, in each iteration of which a multiple kernel learning problem is solved alternatively by multivariate ridge regression and projected gradient descent. Experiments on benchmark data sets indicate that the proposed method outperforms Fisher score as well as many other state-of-the-art feature selection methods.},
 author = {Gu, Quanquan and Li, Zhenhui and Han, Jiawei},
 date = {2012},
 title = {Generalized Fisher Score for Feature Selection},
 publisher = {arXiv},
 doi = {10.48550/arXiv.1202.3725}
}


@article{Heidari.2021,
 abstract = {OBJECTIVE

Since computer-aided diagnosis (CAD) schemes of medical images usually computes large number of image features, which creates a challenge of how to identify a small and optimal feature vector to build robust machine learning models, the objective of this study is to investigate feasibility of applying a random projection algorithm (RPA) to build an optimal feature vector from the initially CAD-generated large feature pool and improve performance of machine learning model.

METHODS

We assemble a retrospective dataset involving 1,487 cases of mammograms in which 644 cases have confirmed malignant mass lesions and 843 have benign lesions. A CAD scheme is first applied to segment mass regions and initially compute 181 features. Then, support vector machine (SVM) models embedded with several feature dimensionality reduction methods are built to predict likelihood of lesions being malignant. All SVM models are trained and tested using a leave-one-case-out cross-validation method. SVM generates a likelihood score of each segmented mass region depicting on one-view mammogram. By fusion of two scores of the same mass depicting on two-view mammograms, a case-based likelihood score is also evaluated.

RESULTS

Comparing with the principle component analyses, nonnegative matrix factorization, and Chi-squared methods, SVM embedded with RPA yielded a significantly higher case-based lesion classification performance with the area under ROC curve of 0.84 $\pm$ 0.01 (p{\textless}0.02).

CONCLUSION

The study demonstrates that RPA is a promising method to generate optimal feature vectors and improve SVM performance.

SIGNIFICANCE

This study presents a new method to develop CAD schemes with significantly higher and robust performance.},
 author = {Heidari, Morteza and Lakshmivarahan, Sivaramakrishnan and Mirniaharikandehei, Seyedehnafiseh and Danala, Gopichandh and Maryada, Sai Kiran R. and Liu, Hong and Zheng, Bin},
 year = {2021},
 title = {Applying a Random Projection Algorithm to Optimize Machine Learning Model for Breast Lesion Classification},
 pages = {2764--2775},
 volume = {68},
 number = {9},
 journal = {IEEE transactions on bio-medical engineering},
 doi = {10.1109/TBME.2021.3054248},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/33493108},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8310536}
}


@incollection{Hertwig.2015,
 author = {Hertwig, Ralph and Pachur, Thorsten},
 title = {Heuristics, History of},
 pages = {829--835},
 publisher = {Elsevier},
 isbn = {9780080970875},
 booktitle = {International Encyclopedia of the Social {\&} Behavioral Sciences},
 year = {2015},
 doi = {10.1016/B978-0-08-097086-8.03221-9}
}


@misc{Heulot.,
 abstract = {As dimensionality increases, analysts are faced with difficult problems to make sense of their data. In exploratory data analysis, multidimensional scaling projections can help analyst to discover patterns by identifying outliers and enabling visual clustering. However to exploit these projections, artifacts and interpretation issues must be overcome. We present ProxiLens, a semantic lens which helps exploring data interactively. The analyst becomes aware of the artifacts navigating in a continuous way through the 2D projection in order to cluster and analyze data. We demonstrate the applicability of our technique for visual clustering on synthetic and real data sets.},
 author = {Heulot, N. and Aupetit, M. and Fekete, J-D.},
 year = {2017},
 title = {ProxiLens: Interactive Exploration of High-Dimensional Data using Projections},
 publisher = {{The Eurographics Association}},
 doi = {10.2312/PE.VAMP.VAMP2013.011-015}
}


@inproceedings{Hinton.2002,
 author = {Hinton, Geoffrey E. and Roweis, Sam},
 title = {Stochastic Neighbor Embedding},
 url = {https://proceedings.neurips.cc/paper/2002/file/6150ccc6069bea6b5716254057a194ef-Paper.pdf},
 volume = {15},
 publisher = {{MIT Press}},
 editor = {{S. Becker} and {S. Thrun} and {K. Obermayer}},
 booktitle = {Advances in Neural Information Processing Systems},
 year = {2002}
}


@inproceedings{Hoffman.1924Oct.1997,
 author = {Hoffman, P. and Grinstein, G. and Marx, K. and Grosse, I. and Stanley, E.},
 title = {DNA visual and analytic data mining},
 pages = {437--441},
 publisher = {IEEE},
 isbn = {0-8186-8262-0},
 booktitle = {Proceedings. Visualization '97 (Cat. No. 97CB36155)},
 year = {19-24 Oct. 1997},
 doi = {10.1109/VISUAL.1997.663916},
 file = {http://ieeexplore.ieee.org/document/663916/}
}


@book{Hutchison.2005,
 year = {2005},
 title = {Artificial Intelligence and Knowledge Engineering Applications: A Bioinspired Approach},
 address = {Berlin, Heidelberg},
 publisher = {{Springer Berlin Heidelberg}},
 isbn = {978-3-540-26319-7},
 series = {Lecture Notes in Computer Science},
 editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and {Pandu Rangan}, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Mira, Jos{\'e} and {\'A}lvarez, Jos{\'e} R.},
 doi = {10.1007/b137296}
}


@article{IBM.1022021,
 abstract = {Learn how data visualization can improve understanding and analyses, enabling better and faster decision making.},
 author = {IBM},
 year = {10/2/2021},
 title = {Data Visualization},
 url = {https://www.ibm.com/cloud/learn/data-visualization},
 urldate = {7/5/2022},
 file = {Data Visualization.pdf}
}


@article{Inselberg.1985,
 author = {Inselberg, Alfred},
 year = {1985},
 title = {The plane with parallel coordinates},
 pages = {69--91},
 volume = {1},
 number = {2},
 issn = {0178-2789},
 journal = {The Visual Computer},
 doi = {10.1007/BF01898350}
}


@book{Inselberg.2009,
 author = {Inselberg, Alfred},
 year = {2009},
 title = {Parallel Coordinates},
 address = {New York, NY},
 publisher = {{Springer New York}},
 isbn = {978-0-387-21507-5},
 doi = {10.1007/978-0-387-68628-8}
}


@book{Kacprzyk.2008,
 year = {2008},
 title = {Hybrid Metaheuristics},
 address = {Berlin, Heidelberg},
 publisher = {{Springer Berlin Heidelberg}},
 isbn = {978-3-540-78294-0},
 series = {Studies in Computational Intelligence},
 editor = {Kacprzyk, Janusz and Blum, Christian and Aguilera, Maria Jos{\'e} Blesa and Roli, Andrea and Sampels, Michael},
 doi = {10.1007/978-3-540-78295-7}
}


@inproceedings{Kandogan.2000,
 author = {Kandogan, Eser},
 title = {Star Coordinates: A Multi-dimensional Visualization Technique with Uniform Treatment of Dimensions},
 pages = {9--12},
 booktitle = {In Proceedings of the IEEE Information Visualization Symposium, Late Breaking Hot Topics},
 year = {2000}
}


@inproceedings{Keim.0507July2006,
 author = {Keim, D. A. and Mansmann, F. and Schneidewind, J. and Ziegler, H.},
 title = {Challenges in Visual Data Analysis},
 pages = {9--16},
 publisher = {IEEE},
 isbn = {0-7695-2602-0},
 booktitle = {Tenth International Conference on Information Visualisation (IV'06)},
 year = {05-07 July 2006},
 doi = {10.1109/IV.2006.31},
 file = {http://ieeexplore.ieee.org/document/1648235/}
}


@incollection{KEIM.2005,
 author = {KEIM, DANIEL A. and Sips, Mike and Ankerst, Mihael},
 title = {Visual Data-Mining Techniques An earlier version of this paper with focus on visualization techniques and their classification has been published in Visual Data Analysis: An Introduction (D. Hand and M. Berthold, Eds.)},
 pages = {831--843},
 publisher = {Elsevier},
 isbn = {9780123875822},
 booktitle = {Visualization Handbook},
 year = {2005},
 doi = {10.1016/B978-012387582-2/50045-9}
}


@book{Keim.2010,
 year = {2010},
 title = {Mastering the information age: Solving problems with visual analytics},
 address = {Goslar},
 publisher = {{Eurographics Association}},
 isbn = {978-3-905673-77-7},
 editor = {Keim, Daniel},
 file = {http://www.gbv.de/dms/tib-ub-hannover/645412244.pdf}
}


@incollection{KEIM.2018,
 author = {KEIM, DANIEL A. and Mansmann, Florian and Stoffel, Andreas and Ziegler, Hartmut},
 title = {Visual Analytics},
 pages = {4463--4469},
 publisher = {{Springer New York}},
 isbn = {978-1-4614-8266-6},
 editor = {Liu, Ling and {\"O}zsu, M. Tamer},
 booktitle = {Encyclopedia of Database Systems},
 year = {2018},
 address = {New York, NY},
 doi = {10.1007/978-1-4614-8265-9{\textunderscore }1122}
}


@article{Kirkpatrick.1983,
 abstract = {There is a deep and useful connection between statistical mechanics (the behavior of systems with many degrees of freedom in thermal equilibrium at a finite temperature) and multivariate or combinatorial optimization (finding the minimum of a given function depending on many parameters). A detailed analogy with annealing in solids provides a framework for optimization of the properties of very large and complex systems. This connection to statistical mechanics exposes new information and provides an unfamiliar perspective on traditional optimization problems and methods.},
 author = {Kirkpatrick, S. and Gelatt, C. D. and Vecchi, M. P.},
 year = {1983},
 title = {Optimization by simulated annealing},
 pages = {671--680},
 volume = {220},
 number = {4598},
 issn = {0036-8075},
 journal = {Science (New York, N.Y.)},
 doi = {10.1126/science.220.4598.671},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/17813860}
}


@inproceedings{Kleinberg.2002,
 author = {Kleinberg, Jon},
 title = {An Impossibility Theorem for Clustering},
 url = {https://proceedings.neurips.cc/paper/2002/file/43e4e6a6f341e00671e123714de019a8-Paper.pdf},
 volume = {15},
 publisher = {{MIT Press}},
 editor = {{S. Becker} and {S. Thrun} and {K. Obermayer}},
 booktitle = {Advances in Neural Information Processing Systems},
 year = {2002}
}


@misc{Lehmann.,
 author = {Lehmann, D. J.},
 title = {Notes on Composition Operators}
}


@article{Lehmann.2013,
 author = {Lehmann, D. J. and Theisel, H.},
 year = {2013},
 title = {Orthographic Star Coordinates},
 journal = {IEEETransactionsonVisualizationandComputerGraphics(ProceedingsIEEEInformationVisualization)}
}


@book{Liu.2018,
 year = {2018},
 title = {Encyclopedia of Database Systems},
 address = {New York, NY},
 publisher = {{Springer New York}},
 isbn = {978-1-4614-8266-6},
 editor = {Liu, Ling and {\"O}zsu, M. Tamer},
 doi = {10.1007/978-1-4614-8265-9}
}


@misc{MarketReportsWorld.08.02.2018,
 author = {{Market Reports World}},
 date = {08.02.2018},
 title = {Global Data Visualisation Market - Segmented by organizational derpartment, delivery mode, industry vertical (BFSI, IT{\&}telecommunication, retail/e-commerce, education, manufacturing, government) and region - growth, trends and forecast (2018-2023)},
 url = {https://www.marketreportsworld.com/global-data-visualization-market-12343651}
}


@misc{McInnes.09.02.2018,
 abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
 author = {McInnes, Leland and Healy, John and Melville, James},
 date = {09.02.2018},
 title = {UMAP: Uniform Manifold Approximation and Projection for Dimension  Reduction},
 url = {http://arxiv.org/pdf/1802.03426v3},
 file = {https://arxiv.org/pdf/1802.03426v3.pdf},
 file = {http://arxiv.org/abs/1802.03426v3}
}


@article{Molchanov.2013,
 author = {Molchanov, Vladimir and Fofonov, Alexey and Linsen, Lars},
 year = {2013},
 title = {Continuous Representation of Projected Attribute Spaces of Multifields over Any Spatial Sampling},
 pages = {301--310},
 volume = {32},
 number = {3pt3},
 issn = {01677055},
 journal = {Computer Graphics Forum},
 doi = {10.1111/cgf.12117}
}


@misc{Molchanov.2014,
 abstract = {Projection methods support effective visualizations of multidimensional data. Linear projections are an importantsubclass, as they allow for interactive visual exploration of the data space and feature sensitivity analysis. Theuser interaction is usually based on an iterative modification of the projection matrix elements, for example, by theuse of a star coordinate widget. However, such interaction mechanisms become inefficient with increasing numberof dimensions. We propose to adapt the projection matrix by allowing the user to directly operate on the projectiondomain. The desired configuration of the projection layout is obtained by adjusting the positions of (freely chosen)control points. The update of the projection matrix is performed according to the interactive modifications bycomputing a least-square solution of a linear equation system. Changes can be tracked easily using animation. Weapply our method to classified multidimensional data and demonstrate that our approach allows for an intuitiveand effective design of projections with desired properties like improved class segregation or reduced clutter.},
 author = {Molchanov, Vladimir and Linsen, Lars},
 year = {2014},
 title = {Interactive Design of Multidimensional Data Projection Layout},
 publisher = {{The Eurographics Association}},
 doi = {10.2312/eurovisshort.20141152}
}


@book{Neil.2001,
 year = {2001},
 title = {International Encyclopedia of the Social {\&} Behavioral Sciences},
 address = {Oxford},
 publisher = {Pergamon},
 isbn = {978-0-08-043076-8},
 editor = {Neil, J. Smelser and Paul, B. Baltes}
}


@incollection{P.M..2001,
 abstract = {Heuristics are approximate strategies or `rules of thumb' for decision making and problem solving that do not guarantee a correct solution but that typically yield a reasonable solution or bring one closer to hand. As such, they stand in contrast to algorithms that will produce a correct solution given complete and correct inputs. More specifically, heuristics are usually thought of as shortcuts that allow decisions or solutions to be reached more rapidly and in conditions of incomplete or uncertain information---often because they do not process all the available information. Decision heuristics have been studied in different research traditions, primarily one that has focused on when and where verbally described heuristics can break down and yield biases, that is, deviations from classical norms of rationality, and another that has investigated how specific computationally modeled heuristicscan exploit structured information to yield fast and accurate decisions. Heuristics proposed for probability judgments include representativeness, availability, and anchoring-and-adjustment; for choices between alternatives, heuristics include recognition, one-reason decision making, and cue tallying; and for sequential search across alternatives, satisficing (searching with an aspiration level) is a common heuristic approach.},
 author = {P.M., Todd},
 title = {Heuristics for Decision and Choice},
 url = {https://www.sciencedirect.com/science/article/pii/B008043076700629X},
 pages = {6676--6679},
 publisher = {Pergamon},
 isbn = {978-0-08-043076-8},
 editor = {Neil, J. Smelser and Paul, B. Baltes},
 booktitle = {International Encyclopedia of the Social {\&} Behavioral Sciences},
 year = {2001},
 address = {Oxford},
 doi = {10.1016/B0-08-043076-7/00629-X}
}


@article{Pandian.1.8.2021,
 abstract = {Data Visualization techniques involve the generation of graphical or pictorial representation~of DATA, to understand the insights},
 author = {Pandian, Shanthababu},
 year = {1.8.2021},
 title = {Effective Data Visualization Techniques in Data Science Using Python},
 url = {https://www.analyticsvidhya.com/blog/2021/08/effective-data-visualization-techniques-in-data-science-using-python/},
 urldate = {18.07.2022},
 journal = {Analytics Vidhya},
 file = {6f0f59d8-b041-4af0-9bcc-efaa1a95570c:C\:\\Users\\Kai\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\qxwt38fbrdocgzokt46z5j1nvvbeqnj7ulmowq6oen45juxms\\Citavi Attachments\\6f0f59d8-b041-4af0-9bcc-efaa1a95570c.pdf:pdf}
}


@proceedings{Ramakrishnan.2000,
 year = {2000},
 title = {Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining  - KDD '00},
 address = {New York, New York, USA},
 publisher = {{ACM Press}},
 isbn = {1581132336},
 editor = {Ramakrishnan, Raghu and Stolfo, Sal and Bayardo, Roberto and Parsa, Ismail},
 doi = {10.1145/347090}
}


@article{Rauber.2018,
 abstract = {Dimensionality reduction is a compelling alternative for high-dimensional data visualization. This method provides insight into high-dimensional feature spaces by mapping relationships between observations (high-dimensional vectors) to low (two or three) dimensional spaces. These low-dimensional representations support tasks such as outlier and group detection based on direct visualization. Supervised learning, a subfield of machine learning, is also concerned with observations. A key task in supervised learning consists of assigning class labels to observations based on generalization from previous experience. Effective development of such classification systems depends on many choices, including features descriptors, learning algorithms, and hyperparameters. These choices are not trivial, and there is no simple recipe to improve classification systems that perform poorly. In this context, we first propose the use of visual representations based on dimensionality reduction (projections) for predictive feedback on classification efficacy. Second, we propose a projection-based visual analytics methodology, and supportive tooling, that can be used to improve classification systems through feature selection. We evaluate our proposal through experiments involving four datasets and three representative learning algorithms.},
 author = {Rauber, Paulo E. and Falc{\~a}o, Alexandre X. and Telea, Alexandru C.},
 year = {2018},
 title = {Projections as visual aids for classification system design},
 pages = {282--305},
 volume = {17},
 number = {4},
 issn = {1473-8716},
 journal = {Information visualization},
 doi = {10.1177/1473871617713337}
}


@article{RubioSanchez.2016,
 abstract = {RadViz and star coordinates are two of the most popular projection-based multivariate visualization techniques that arrange variables in radial layouts. Formally, the main difference between them consists of a nonlinear normalization step inherent in RadViz. In this paper we show that, although RadViz can be useful when analyzing sparse data, in general this design choice limits its applicability and introduces several drawbacks for exploratory data analysis. In particular, we observe that the normalization step introduces nonlinear distortions, can encumber outlier detection, prevents associating the plots with useful linear mappings, and impedes estimating original data attributes accurately. In addition, users have greater flexibility when choosing different layouts and views of the data in star coordinates. Therefore, we suggest that analysts and researchers should carefully consider whether RadViz's normalization step is beneficial regarding the data sets' characteristics and analysis tasks.},
 author = {Rubio-S{\'a}nchez, Manuel and Raya, Laura and D{\'i}az, Francisco and Sanchez, Alberto},
 year = {2016},
 title = {A comparative study between RadViz and Star Coordinates},
 pages = {619--628},
 volume = {22},
 number = {1},
 journal = {IEEE transactions on visualization and computer graphics},
 doi = {10.1109/TVCG.2015.2467324},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/26529721}
}


@proceedings{S.Becker.2002,
 year = {2002},
 title = {Advances in Neural Information Processing Systems},
 publisher = {{MIT Press}},
 editor = {{S. Becker} and {S. Thrun} and {K. Obermayer}}
}


@proceedings{S.Becker.2002b,
 year = {2002},
 title = {Advances in Neural Information Processing Systems},
 publisher = {{MIT Press}},
 editor = {{S. Becker} and {S. Thrun} and {K. Obermayer}}
}


@article{Sammon.1969,
 author = {Sammon, J. W.},
 year = {1969},
 title = {A nonlinear mapping for data structure analysis},
 pages = {401--409},
 volume = {18},
 number = {5},
 journal = {IEEE Transactions on Computers}
}


@article{Sedlmair.2013,
 abstract = {To verify cluster separation in high-dimensional data, analysts often reduce the data with a dimension reduction (DR) technique, and then visualize it with 2D Scatterplots, interactive 3D Scatterplots, or Scatterplot Matrices (SPLOMs). With the goal of providing guidance between these visual encoding choices, we conducted an empirical data study in which two human coders manually inspected a broad set of 816 scatterplots derived from 75 datasets, 4 DR techniques, and the 3 previously mentioned scatterplot techniques. Each coder scored all color-coded classes in each scatterplot in terms of their separability from other classes. We analyze the resulting quantitative data with a heatmap approach, and qualitatively discuss interesting scatterplot examples. Our findings reveal that 2D scatterplots are often 'good enough', that is, neither SPLOM nor interactive 3D adds notably more cluster separability with the chosen DR technique. If 2D is not good enough, the most promising approach is to use an alternative DR technique in 2D. Beyond that, SPLOM occasionally adds additional value, and interactive 3D rarely helps but often hurts in terms of poorer class separation and usability. We summarize these results as a workflow model and implications for design. Our results offer guidance to analysts during the DR exploration process.},
 author = {Sedlmair, Michael and Munzner, Tamara and Tory, Melanie},
 year = {2013},
 title = {Empirical guidance on scatterplot and dimension reduction technique choices},
 pages = {2634--2643},
 volume = {19},
 number = {12},
 journal = {IEEE transactions on visualization and computer graphics},
 doi = {10.1109/TVCG.2013.153},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/24051830}
}


@article{Sips.2009,
 author = {Sips, Mike and Neubert, Boris and Lewis, John P. and Hanrahan, Pat},
 year = {2009},
 title = {Selecting good views of high-dimensional data using class consistency},
 pages = {831--838},
 volume = {28},
 number = {3},
 issn = {01677055},
 journal = {Computer Graphics Forum},
 doi = {10.1111/j.1467-8659.2009.01467.x}
}


@article{Sorensen.2015,
 author = {S{\"o}rensen, Kenneth},
 year = {2015},
 title = {Metaheuristics-the metaphor exposed},
 pages = {3--18},
 volume = {22},
 number = {1},
 issn = {09696016},
 journal = {International Transactions in Operational Research},
 doi = {10.1111/itor.12001}
}


@article{Tenenbaum.2000,
 abstract = {Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations. The human brain confronts the same problem in everyday perception, extracting from its high-dimensional sensory inputs-30,000 auditory nerve fibers or 10(6) optic nerve fibers-a manageably small number of perceptually relevant features. Here we describe an approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set. Unlike classical techniques such as principal component analysis (PCA) and multidimensional scaling (MDS), our approach is capable of discovering the nonlinear degrees of freedom that underlie complex natural observations, such as human handwriting or images of a face under different viewing conditions. In contrast to previous algorithms for nonlinear dimensionality reduction, ours efficiently computes a globally optimal solution, and, for an important class of data manifolds, is guaranteed to converge asymptotically to the true structure.},
 author = {Tenenbaum, J. B. and de Silva, V. and Langford, J. C.},
 year = {2000},
 title = {A global geometric framework for nonlinear dimensionality reduction},
 pages = {2319--2323},
 volume = {290},
 number = {5500},
 issn = {0036-8075},
 journal = {Science (New York, N.Y.)},
 doi = {10.1126/science.290.5500.2319},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/11125149}
}


@inproceedings{Teoh.2003,
 author = {Teoh, Soon Tee and Ma, Kwan-Liu},
 title = {StarClass: Interactive Visual Classification Using Star Coordinates},
 pages = {178--185},
 publisher = {{Society for Industrial and Applied Mathematics}},
 isbn = {978-0-89871-545-3},
 editor = {Barbara, Daniel and Kamath, Chandrika},
 booktitle = {Proceedings of the 2003 SIAM International Conference on Data Mining},
 year = {2003},
 address = {Philadelphia, PA},
 doi = {10.1137/1.9781611972733.16},
 file = {https://epubs.siam.org/doi/book/10.1137/1.9781611972733}
}


@article{vanderMaaten.2008,
 author = {{van der Maaten}, Laurens and Hinton, Geoffrey},
 year = {2008},
 title = {Viualizing data using t-SNE},
 pages = {2579--2605},
 volume = {9},
 journal = {Journal of Machine Learning Research}
}


@book{Zimmermann.2005,
 abstract = {Einf{\"u}hrung -- 1 Die Geschichte des Operations Research -- 2 Entscheidungs- und Spieltheorie -- 3 Lineares Programmieren -- 4 Nichtlineare Programmierung -- 5 Entscheidungsbaumverfahren -- 6 Heuristische Verfahren -- 7 Ganzzahlige Lineare Programmierung -- 8 Graphen, B{\"a}ume, Netze, Netzpl{\"a}ne -- 9 Theorie der Warteschlangen -- L{\"o}sungen der Aufgaben.



In diesem Buch finden Sie die Br{\"u}cke zwischen klassischem Operations Research und den modernen Gebieten der Heuristik und der Theorie unscharfer Mengen. Klassische und moderne Verfahren und Modelle der Unternehmensforschung sind didaktisch geschickt dargestellt. Das Buch ist entscheidungs- und EDV-orientiert. Mit besonderen Kapiteln {\"u}ber Heuristiken, ganzzahlige Programmierung und die Modellierung schlecht strukturierter Systeme dient es Ihnen als modernes Lehr- und Nachschlagewerk.},
 author = {Zimmermann, Hans-J{\"u}rgen},
 year = {2005},
 title = {Operations Research: Methoden und Modelle. F{\"u}r Wirtschaftsingenieure, Betriebswirte, Informatiker},
 address = {Wiesbaden},
 publisher = {{Vieweg+Teubner Verlag}},
 isbn = {978-3-322-93906-7},
 series = {Springer eBook Collection},
 doi = {10.1007/978-3-322-93906-7},
 file = {https://swbplus.bsz-bw.de/bsz116859768cov.jpg},
 file = {https://zbmath.org/?q=an:1137.90301}
}


